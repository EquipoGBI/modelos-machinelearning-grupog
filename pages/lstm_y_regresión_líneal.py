# -*- coding: utf-8 -*-
"""Copia de LSTM_y_Regresión_Líneal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EwcIXK-H4unh1UibpkgatSeFwRFKOGp-

# PARTE I: LSTM
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/Kulbear/stock-prediction
# %cd stock-prediction

"""## Predicción de acciones con red neuronal recurrente
### Se realizará un LSTM simple con Keras para predecir el precio de las acciones en la bolsa china.
"""

import time
import math
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import LSTM
import numpy as np
import pandas as pd
import sklearn.preprocessing as prep

"""## Datos de importacion
### Presentación de cómo obtenemos el conjunto de datos
"""

df = pd.read_csv('000002-from-1995-01-01.csv')
df.head()

"""## Preprocesar datos

### Podemos ver en la tabla de arriba que las escalas de las características varían mucho. Se procederá a estandarizar los datos
"""

def standard_scaler(X_train, X_test):
    train_samples, train_nx, train_ny = X_train.shape
    test_samples, test_nx, test_ny = X_test.shape
    
    X_train = X_train.reshape((train_samples, train_nx * train_ny))
    X_test = X_test.reshape((test_samples, test_nx * test_ny))
    
    preprocessor = prep.StandardScaler().fit(X_train)
    X_train = preprocessor.transform(X_train)
    X_test = preprocessor.transform(X_test)
    
    X_train = X_train.reshape((train_samples, train_nx, train_ny))
    X_test = X_test.reshape((test_samples, test_nx, test_ny))
    
    return X_train, X_test

"""### División de los datos en X_train, y_train, X_test, y_test"""

def preprocess_data(stock, seq_len):
    amount_of_features = len(stock.columns)
    data = stock.values
    
    sequence_length = seq_len + 1
    result = []
    for index in range(len(data) - sequence_length):
        result.append(data[index : index + sequence_length])
        
    result = np.array(result)
    row = round(0.9 * result.shape[0])
    train = result[: int(row), :]
    
    train, result = standard_scaler(train, result)
    
    X_train = train[:, : -1]
    y_train = train[:, -1][: ,-1]
    X_test = result[int(row) :, : -1]
    y_test = result[int(row) :, -1][ : ,-1]

    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  

    return [X_train, y_train, X_test, y_test]

"""## Construcción de la red LSTM

### Construcción de un RNN simple con 2 capas LSTM.
### La arquitectura es:
    
### LSTM --> Abandono --> LSTM --> Abandono --> Totalmente Conectado (Denso)
"""

def build_model(layers):
    model = Sequential()
    model.add(LSTM(
        input_dim=layers[0],
        units=layers[1],
        return_sequences=True))
    model.add(Dropout(0.4))

    model.add(LSTM(
        layers[2],
        return_sequences=False))
    model.add(Dropout(0.3))

    model.add(Dense(
        units=layers[3]))
    model.add(Activation("linear"))

    start = time.time()
    model.compile(loss="mse", optimizer="rmsprop", metrics=['accuracy'])
    print("Compilation Time : ", time.time() - start)
    return model

window = 20
X_train, y_train, X_test, y_test = preprocess_data(df[:: -1], window)
print("X_train", X_train.shape)
print("y_train", y_train.shape)
print("X_test", X_test.shape)
print("y_test", y_test.shape)

model = build_model([X_train.shape[2], window, 100, 1])

"""
## Capacitación de la red

### El proceso de entrenamiento puede llevar mucho tiempo si se está utilizando una GPU o CPU promedio."""

model.fit(
    X_train,
    y_train,
    batch_size=768,
    epochs=5,
    validation_split=0.1,
    verbose=0)

trainScore = model.evaluate(X_train, y_train, verbose=0)
print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))
testScore = model.evaluate(X_test, y_test, verbose=0)
print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))

"""### Visualización de la predicción"""

diff = []
ratio = []
pred = model.predict(X_test)
for u in range(len(y_test)):
    pr = pred[u][0]
    ratio.append((y_test[u] / pr) - 1)
    diff.append(abs(y_test[u] - pr))

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 

import matplotlib
import matplotlib.pyplot as plt2

plt2.plot(pred, color='red', label='Prediction')
plt2.plot(y_test, color='blue', label='Ground Truth')
plt2.legend(loc='upper left')
plt2.show()

"""# PARTE II: Regresión Lineal"""

!pip install quandl

import pandas as pd
import matplotlib.pyplot as plt 
import quandl 
from sklearn.linear_model import LinearRegression

df.isnull().sum()

import seaborn as sns
plt.figure(1 , figsize = (17 , 8))
cor = sns.heatmap(df.corr(), annot = True)

x = df.loc[:,'high':'close']
y = df.loc[:,'open']

x.head()

y.head()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.1,random_state = 101)

LR = LinearRegression()

LR.fit(x_train,y_train)

LR.score(x_test,y_test)

#Evaluación del Modelo
#Se evalua el modelo comprobando sus coeficientes.
#Se imprime el interceptor
print(LR.intercept_)

coeff_df = pd.DataFrame(LR.coef_,x.columns,columns=['Coeficiente'])
coeff_df

#Predicciones del modelo

x_test.head()

predictions = LR.predict(x_test)
predictions

plt.scatter(y_test,predictions)

sns.displot((y_test-predictions),bins=50);

#MÉTRICAS

from sklearn import metrics

print('MAE:', metrics.mean_absolute_error(y_test, predictions))
print('MSE:', metrics.mean_squared_error(y_test, predictions))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.plot(predictions,color='red')